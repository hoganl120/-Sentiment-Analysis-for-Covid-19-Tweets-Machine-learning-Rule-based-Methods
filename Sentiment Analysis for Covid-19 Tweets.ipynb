{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf476aa",
   "metadata": {},
   "source": [
    "# Data for Model Building and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1481d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbefc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import cleantext\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb \n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.utils.deprecation import _raise_dep_warning_if_not_pytest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355758d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                              ]     0 / 10246\r",
      " 79% [..............................................................                ]  8192 / 10246\r",
      "100% [..............................................................................] 10246 / 10246"
     ]
    }
   ],
   "source": [
    "wget.download('https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py')\n",
    "\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac32c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 607343 / 607343"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nlp_getting_started (2).zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url =\"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2310a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c08aff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text  target  \n",
       "0  I`d have responded, if I were going       0  \n",
       "1                             Sooo SAD       1  \n",
       "2                          bullying me       1  \n",
       "3                       leave me alone       1  \n",
       "4                        Sons of ****,       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names = ['target','ids','date','flag','user','text'])\n",
    "\n",
    "train_df = pd.read_excel('trainp.xlsx')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96477eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27446 non-null  object\n",
      " 3   target         27481 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 858.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#train_df.target = train_df.target.astype(int)\n",
    "#train_df[\"target\"] = pd.to_numeric(train_df[\"target\"], downcast=\"float\", errors='coerce')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96f154e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    19700\n",
       "1.0     7781\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c46bb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text=train_df.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e198cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jelee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jelee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jelee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # miniscule\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove emoji\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # removing usernames\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # removing links\n",
    "    text = re.sub('((https?://[^\\s]+)|(www\\.[^\\s]+))','',text)\n",
    "    \n",
    "        # removing rt\n",
    "    text = re.sub('rt','',text)\n",
    "    \n",
    "    # removing punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "   \n",
    "    # suppression of stop words and lemmatization\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    final_words = []\n",
    "    for token in tokens:\n",
    "      # suppression of stop words\n",
    "      if token not in stop_words and len(token) > 1:\n",
    "        lemm = wordLemm.lemmatize(token)\n",
    "        final_words.append(lemm)\n",
    "    return ' '.join(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "050dd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleantext'] = train_df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf2c8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id responded going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bos bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>1.0</td>\n",
       "      <td>son couldnt put release already bought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text  target  \\\n",
       "0  I`d have responded, if I were going     0.0   \n",
       "1                             Sooo SAD     1.0   \n",
       "2                          bullying me     1.0   \n",
       "3                       leave me alone     1.0   \n",
       "4                        Sons of ****,     1.0   \n",
       "\n",
       "                                cleantext  \n",
       "0                      id responded going  \n",
       "1                 sooo sad miss san diego  \n",
       "2                            bos bullying  \n",
       "3                   interview leave alone  \n",
       "4  son couldnt put release already bought  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "881647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Clean\n",
    "import pandas as pd\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea1e7763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean training text\n",
    "l=len(train_df)\n",
    "display(l)\n",
    "cleanlist=[]\n",
    "textlength=[]\n",
    "for i in range(l):\n",
    "    ct=cleantext.clean(train_df.iloc[i,3], clean_all= True)\n",
    "    cleanlist.append(ct)\n",
    "    lct=len(ct)\n",
    "    textlength.append(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59572aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>ariaahrari thetawniest control wild fire calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>utckm volcano hawaii httptcozdtoydebj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>polic investig ebik collid car littl portug eb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                              cleantext  \n",
       "0             deed reason earthquak may allah forgiv us  \n",
       "1                  forest fire near la rong sask canada  \n",
       "2     resid ask shelter place notifi offic evacu she...  \n",
       "3           peopl receiv wildfir evacu order california  \n",
       "4     got sent photo rubi alaska smoke wildfir pour ...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane hold bridg collaps nearbi home...  \n",
       "7609  ariaahrari thetawniest control wild fire calif...  \n",
       "7610              utckm volcano hawaii httptcozdtoydebj  \n",
       "7611  polic investig ebik collid car littl portug eb...  \n",
       "7612  latest home raze northern california wildfir a...  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean=pd.DataFrame(cleanlist)\n",
    "df_clean.columns=['cleantext']\n",
    "frames=[train_df,df_clean]\n",
    "newdf=pd.concat(frames, axis=1)\n",
    "display(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0df58ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>a7f72a928a</td>\n",
       "      <td>WOOOOOOOOOO   are you coming to Nottingham at...</td>\n",
       "      <td>t?  lovelovelove</td>\n",
       "      <td>0</td>\n",
       "      <td>woooooooooo coming nottingham point lovelovelove3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23879</th>\n",
       "      <td>ef42dee96c</td>\n",
       "      <td>resting had a whole day of walking</td>\n",
       "      <td>resting had a whole day of walking</td>\n",
       "      <td>0</td>\n",
       "      <td>resting whole day walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>07d17131b1</td>\n",
       "      <td>was in Palawan a couple of days ago, i`ll try ...</td>\n",
       "      <td>was in Palawan a couple of days ago, i`ll try ...</td>\n",
       "      <td>0</td>\n",
       "      <td>palawan couple day ago ill try post picture tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>2820205db5</td>\n",
       "      <td>I know! I`m so slow its horrible. DON`T TELL ...</td>\n",
       "      <td>horrible.</td>\n",
       "      <td>1</td>\n",
       "      <td>know im slow horrible dont tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>7d3ce4363c</td>\n",
       "      <td>Glad I went out, glad I didn`t leave early, an...</td>\n",
       "      <td>glad</td>\n",
       "      <td>0</td>\n",
       "      <td>glad went glad didnt leave early glad afterpay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "1588   a7f72a928a   WOOOOOOOOOO   are you coming to Nottingham at...   \n",
       "23879  ef42dee96c                 resting had a whole day of walking   \n",
       "6561   07d17131b1  was in Palawan a couple of days ago, i`ll try ...   \n",
       "2602   2820205db5   I know! I`m so slow its horrible. DON`T TELL ...   \n",
       "4003   7d3ce4363c  Glad I went out, glad I didn`t leave early, an...   \n",
       "\n",
       "                                           selected_text  target  \\\n",
       "1588                                    t?  lovelovelove       0   \n",
       "23879                 resting had a whole day of walking       0   \n",
       "6561   was in Palawan a couple of days ago, i`ll try ...       0   \n",
       "2602                                           horrible.       1   \n",
       "4003                                                glad       0   \n",
       "\n",
       "                                               cleantext  \n",
       "1588   woooooooooo coming nottingham point lovelovelove3  \n",
       "23879                          resting whole day walking  \n",
       "6561     palawan couple day ago ill try post picture tom  \n",
       "2602                     know im slow horrible dont tell  \n",
       "4003   glad went glad didnt leave early glad afterpay...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ad83810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"cleantext\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e7074ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24732, 24732, 2749, 2749)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "526e3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
    "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
    "\n",
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6002ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ae939f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x212735ebd90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431c911",
   "metadata": {},
   "source": [
    "# Model0: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e59be95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b1893fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bd0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a988d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "  -----\n",
    "  y_true = true labels in the form of a 1D array\n",
    "  y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a094d4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.31866133139323,\n",
       " 'precision': 0.7923656577921231,\n",
       " 'recall': 0.7631866133139323,\n",
       " 'f1': 0.6942348800890389}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8cb86",
   "metadata": {},
   "source": [
    "# Model_0_1:XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dc2a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0_1 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", xgb.XGBClassifier()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0_1.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c121fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline01_preds = model_0_1.predict(val_sentences)\n",
    "baseline01_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "219ced2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 82.06620589305203,\n",
       " 'precision': 0.8168084487553012,\n",
       " 'recall': 0.8206620589305202,\n",
       " 'f1': 0.8033551638746379}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline01_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline01_preds)\n",
    "baseline01_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613d171",
   "metadata": {},
   "source": [
    "# RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e9971d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58699f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf', RidgeClassifier(class_weight='balanced'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenization and modelling pipeline\n",
    "model_0_2 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", linear_model.RidgeClassifier(class_weight='balanced')) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0_2.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7f6405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 69.84357948344852,\n",
       " 'precision': 0.780520237628925,\n",
       " 'recall': 0.6984357948344853,\n",
       " 'f1': 0.7149417345145571}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline02_preds = model_0_2.predict(val_sentences)\n",
    "baseline02_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline02_preds)\n",
    "baseline02_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de4d2a",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd30610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53c491c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf', MLPClassifier(max_iter=300, random_state=42))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenization and modelling pipeline\n",
    "model_0_3 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MLPClassifier(max_iter=300, random_state=42)) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0_3.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60f9bd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.01054929065114,\n",
       " 'precision': 0.7912733684492507,\n",
       " 'recall': 0.7901054929065114,\n",
       " 'f1': 0.7906703375255775}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline03_preds = model_0_3.predict(val_sentences)\n",
    "baseline03_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline03_preds)\n",
    "baseline03_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455086d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57937989",
   "metadata": {},
   "source": [
    "# USE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "700bf2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 210.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 420.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 620.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 840.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a30cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "USE (KerasLayer)             (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Create model using the Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "], name=\"model_6_USE\")\n",
    "\n",
    "# Compile model\n",
    "model_6.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0540d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "773/773 [==============================] - 131s 152ms/step - loss: 0.4462 - accuracy: 0.7927 - val_loss: 0.4061 - val_accuracy: 0.8192\n",
      "Epoch 2/10\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.4056 - accuracy: 0.8147 - val_loss: 0.4004 - val_accuracy: 0.8210\n",
      "Epoch 3/10\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3961 - accuracy: 0.8195 - val_loss: 0.3998 - val_accuracy: 0.8196\n",
      "Epoch 4/10\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.3865 - accuracy: 0.8269 - val_loss: 0.3975 - val_accuracy: 0.8247\n",
      "Epoch 5/10\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.3767 - accuracy: 0.8316 - val_loss: 0.3958 - val_accuracy: 0.8236\n",
      "Epoch 6/10\n",
      "773/773 [==============================] - 6s 8ms/step - loss: 0.3667 - accuracy: 0.8388 - val_loss: 0.3961 - val_accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.3577 - accuracy: 0.8437 - val_loss: 0.3965 - val_accuracy: 0.8250\n",
      "Epoch 8/10\n",
      "773/773 [==============================] - 5s 7ms/step - loss: 0.3486 - accuracy: 0.8488 - val_loss: 0.3975 - val_accuracy: 0.8272\n",
      "Epoch 9/10\n",
      "773/773 [==============================] - 5s 6ms/step - loss: 0.3400 - accuracy: 0.8538 - val_loss: 0.3979 - val_accuracy: 0.8247\n",
      "Epoch 10/10\n",
      "773/773 [==============================] - 5s 6ms/step - loss: 0.3305 - accuracy: 0.8585 - val_loss: 0.3989 - val_accuracy: 0.8258\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of pretrained embeddings\n",
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=10,\n",
    "                              validation_data=(val_sentences, val_labels)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3840c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with USE TF Hub model\n",
    "model_6_pred_probs = model_6.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ac960e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04e86da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 82.57548199345216,\n",
       " 'precision': 0.8196212938957227,\n",
       " 'recall': 0.8257548199345216,\n",
       " 'f1': 0.8210058029142369}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_6_results = calculate_results(val_labels, model_6_preds)\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2018eb",
   "metadata": {},
   "source": [
    "# Feed-forward neural network (dense model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fa763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b062ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "163913ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "527ce11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20220302-121615\n",
      "Epoch 1/5\n",
      "773/773 [==============================] - 18s 20ms/step - loss: 0.5158 - accuracy: 0.7489 - val_loss: 0.4294 - val_accuracy: 0.8025\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 14s 18ms/step - loss: 0.3751 - accuracy: 0.8402 - val_loss: 0.4039 - val_accuracy: 0.8236\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 17s 22ms/step - loss: 0.3217 - accuracy: 0.8688 - val_loss: 0.4135 - val_accuracy: 0.8210\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 14s 18ms/step - loss: 0.2905 - accuracy: 0.8854 - val_loss: 0.4322 - val_accuracy: 0.8192\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 13s 17ms/step - loss: 0.2676 - accuracy: 0.8962 - val_loss: 0.4492 - val_accuracy: 0.8178\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf1cbed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.77519097853765,\n",
       " 'precision': 0.8103476894663983,\n",
       " 'recall': 0.8177519097853765,\n",
       " 'f1': 0.8116572297344858}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0af08",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f8c3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa0e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d375bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/LSTM/20220303-101847\n",
      "Epoch 1/5\n",
      "773/773 [==============================] - 130s 137ms/step - loss: 0.4513 - accuracy: 0.7978 - val_loss: 0.4348 - val_accuracy: 0.8138\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 21s 27ms/step - loss: 0.3283 - accuracy: 0.8648 - val_loss: 0.4031 - val_accuracy: 0.8192\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 21s 27ms/step - loss: 0.2668 - accuracy: 0.8939 - val_loss: 0.4959 - val_accuracy: 0.8163\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 23s 30ms/step - loss: 0.2130 - accuracy: 0.9150 - val_loss: 0.5097 - val_accuracy: 0.8090\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 22s 28ms/step - loss: 0.1706 - accuracy: 0.9310 - val_loss: 0.6188 - val_accuracy: 0.8007\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec897577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.06547835576573,\n",
       " 'precision': 0.7969378617204406,\n",
       " 'recall': 0.8006547835576573,\n",
       " 'f1': 0.7985217060804096}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "\n",
    "model_2_results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be162f5",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89197801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_3\")\n",
    "\n",
    "# Build an RNN using the GRU cell\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n",
    "x = layers.GRU(64)(x) \n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b75d133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile GRU model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ec5a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/GRU/20220303-102406\n",
      "Epoch 1/5\n",
      "773/773 [==============================] - 27s 28ms/step - loss: 0.4641 - accuracy: 0.7891 - val_loss: 0.4145 - val_accuracy: 0.8236\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 19s 24ms/step - loss: 0.3276 - accuracy: 0.8658 - val_loss: 0.3960 - val_accuracy: 0.8247\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 18s 23ms/step - loss: 0.2604 - accuracy: 0.8976 - val_loss: 0.4716 - val_accuracy: 0.8203\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 18s 23ms/step - loss: 0.1942 - accuracy: 0.9228 - val_loss: 0.5369 - val_accuracy: 0.8094\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 16s 21ms/step - loss: 0.1484 - accuracy: 0.9398 - val_loss: 0.6473 - val_accuracy: 0.8043\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_3_history = model_3.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "663bc820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.4292469989087,\n",
       " 'precision': 0.7980248973105916,\n",
       " 'recall': 0.804292469989087,\n",
       " 'f1': 0.8001609517020097}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation data\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa1258",
   "metadata": {},
   "source": [
    "# Predict data with 3 models built\n",
    "Naive Bayes\n",
    "USE\n",
    "Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "230da997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>usa</th>\n",
       "      <th>usar</th>\n",
       "      <th>nevent</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "      <th>BERT</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @EbonyT_PhD:Shout out to the nurses who???Â€...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GOPKeegan: True! #Democrats #COVID19 #Trut...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @nilikm:In other words, daily #COVID19  rep...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Mississauga, Ontario</td>\n",
       "      <td>1280000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.6440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>It???Â€Â€?Â€Â€?s Wednesday. _x000D_\\n_x000D_\\nScho...</td>\n",
       "      <td>Wed Dec 29 18:01:41 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>at least 2m away from you</td>\n",
       "      <td>26824287</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @VikiLovesFACS:How can we know that #COVID1...</td>\n",
       "      <td>Wed Dec 29 18:01:42 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1330000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>il</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  RT @EbonyT_PhD:Shout out to the nurses who???Â€...   \n",
       "1   1  RT @GOPKeegan: True! #Democrats #COVID19 #Trut...   \n",
       "2   2  RT @nilikm:In other words, daily #COVID19  rep...   \n",
       "3   3  It???Â€Â€?Â€Â€?s Wednesday. _x000D_\\n_x000D_\\nScho...   \n",
       "4   4  RT @VikiLovesFACS:How can we know that #COVID1...   \n",
       "\n",
       "                       created_at lang                   location  \\\n",
       "0  Wed Dec 29 18:01:40 +0000 2021   en                        NaN   \n",
       "1  Wed Dec 29 18:01:40 +0000 2021   en                        NaN   \n",
       "2  Wed Dec 29 18:01:40 +0000 2021   en       Mississauga, Ontario   \n",
       "3  Wed Dec 29 18:01:41 +0000 2021   en  at least 2m away from you   \n",
       "4  Wed Dec 29 18:01:42 +0000 2021   en                Chicago, IL   \n",
       "\n",
       "                 user_id    event  usa  usar  nevent  ... joy  sadness  \\\n",
       "0   718000000000000000.0  omicron  NaN   NaN     2.0  ...   0        0   \n",
       "1  1450000000000000000.0  omicron  NaN   NaN     2.0  ...   1        0   \n",
       "2  1280000000000000000.0  omicron  NaN   0.0     2.0  ...   0        0   \n",
       "3               26824287  omicron  NaN   0.0     2.0  ...   0        0   \n",
       "4  1330000000000000000.0  omicron   il   1.0     2.0  ...   0        0   \n",
       "\n",
       "   surprise  trust  negative  positive  sentiment   vader  BERT  text_len  \n",
       "0         0      1         1         3   0.113636  0.8523   0.0       176  \n",
       "1         0      2         0         2   0.350000  0.6588   0.0        31  \n",
       "2         0      3         2         4   0.000000 -0.6440   0.0       158  \n",
       "3         0      1         0         0  -0.600000  0.0000   0.0        76  \n",
       "4         0      0         1         0   0.050000  0.5873   0.0        79  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('f:/cvds.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6bf4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ct=np.asarray(df['cleantext'].astype(str))\n",
    "\n",
    "model_1_pred_probs = model_1.predict(ct)\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "\n",
    "model_0_1preds = model_0_1.predict(ct)\n",
    "\n",
    "# Make predictions with USE TF Hub model\n",
    "model_6_pred_probs = model_6.predict(ct)\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs)) # squeeze removes single dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ac278ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103620"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_0_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6abd603d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>usa</th>\n",
       "      <th>usar</th>\n",
       "      <th>nevent</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "      <th>BERT</th>\n",
       "      <th>text_len</th>\n",
       "      <th>XG_p</th>\n",
       "      <th>DENSE_p</th>\n",
       "      <th>USE_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @EbonyT_PhD:Shout out to the nurses who???Â€...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GOPKeegan: True! #Democrats #COVID19 #Trut...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @nilikm:In other words, daily #COVID19  rep...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Mississauga, Ontario</td>\n",
       "      <td>1280000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.6440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>It???Â€Â€?Â€Â€?s Wednesday. _x000D_\\n_x000D_\\nScho...</td>\n",
       "      <td>Wed Dec 29 18:01:41 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>at least 2m away from you</td>\n",
       "      <td>26824287</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @VikiLovesFACS:How can we know that #COVID1...</td>\n",
       "      <td>Wed Dec 29 18:01:42 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1330000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>il</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  RT @EbonyT_PhD:Shout out to the nurses who???Â€...   \n",
       "1   1  RT @GOPKeegan: True! #Democrats #COVID19 #Trut...   \n",
       "2   2  RT @nilikm:In other words, daily #COVID19  rep...   \n",
       "3   3  It???Â€Â€?Â€Â€?s Wednesday. _x000D_\\n_x000D_\\nScho...   \n",
       "4   4  RT @VikiLovesFACS:How can we know that #COVID1...   \n",
       "\n",
       "                       created_at lang                   location  \\\n",
       "0  Wed Dec 29 18:01:40 +0000 2021   en                        NaN   \n",
       "1  Wed Dec 29 18:01:40 +0000 2021   en                        NaN   \n",
       "2  Wed Dec 29 18:01:40 +0000 2021   en       Mississauga, Ontario   \n",
       "3  Wed Dec 29 18:01:41 +0000 2021   en  at least 2m away from you   \n",
       "4  Wed Dec 29 18:01:42 +0000 2021   en                Chicago, IL   \n",
       "\n",
       "                 user_id    event  usa  usar  nevent  ... trust  negative  \\\n",
       "0   718000000000000000.0  omicron  NaN   NaN     2.0  ...     1         1   \n",
       "1  1450000000000000000.0  omicron  NaN   NaN     2.0  ...     2         0   \n",
       "2  1280000000000000000.0  omicron  NaN   0.0     2.0  ...     3         2   \n",
       "3               26824287  omicron  NaN   0.0     2.0  ...     1         0   \n",
       "4  1330000000000000000.0  omicron   il   1.0     2.0  ...     0         1   \n",
       "\n",
       "   positive  sentiment   vader  BERT  text_len  XG_p  DENSE_p  USE_p  \n",
       "0         3   0.113636  0.8523   0.0       176     1      0.0    0.0  \n",
       "1         2   0.350000  0.6588   0.0        31     0      0.0    0.0  \n",
       "2         4   0.000000 -0.6440   0.0       158     1      1.0    0.0  \n",
       "3         0  -0.600000  0.0000   0.0        76     0      1.0    0.0  \n",
       "4         0   0.050000  0.5873   0.0        79     0      0.0    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1=pd.DataFrame(model_0_1preds)\n",
    "df1.columns=['XG_p']\n",
    "df2=pd.DataFrame(model_1_preds)\n",
    "df2.columns=['DENSE_p']\n",
    "df3=pd.DataFrame(model_6_preds)\n",
    "df3.columns=['USE_p']\n",
    "\n",
    "frames=[df,df1, df2, df3 ]\n",
    "df=pd.concat(frames, axis=1)\n",
    "display(df.head())andas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1fa84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('f:\\df_sent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd71c20",
   "metadata": {},
   "source": [
    "# Rule_based Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "595e9371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (1,6,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>usa</th>\n",
       "      <th>usar</th>\n",
       "      <th>nevent</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>MNB</th>\n",
       "      <th>DENSE</th>\n",
       "      <th>USE</th>\n",
       "      <th>BERT</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @EbonyT_PhD:Shout out to the nurses who??Â€?...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nurse who?Â€ve told can?Â€t drink water nurse st...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @GOPKeegan: True! #Democrats #COVID19 #Trut...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>true democrat covid19 truth covid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @nilikm:In other words, daily #COVID19  rep...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Mississauga, Ontario</td>\n",
       "      <td>1280000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>word daily covid19 repos gross underestimate t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...</td>\n",
       "      <td>Wed Dec 29 18:01:41 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>at least 2m away from you</td>\n",
       "      <td>26824287.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>it?Â€s wednesday school stas monday one 430pm f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RT @VikiLovesFACS:How can we know that #COVID1...</td>\n",
       "      <td>Wed Dec 29 18:01:42 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1330000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>il</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>know covid19 vaccine harm feility could effect...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 0.1.1                                               text  ... trust negative positive\n",
       "0           0              0  RT @EbonyT_PhD:Shout out to the nurses who??Â€?...  ...     1        1        3\n",
       "1           1              1  RT @GOPKeegan: True! #Democrats #COVID19 #Trut...  ...     2        0        2\n",
       "2           2              2  RT @nilikm:In other words, daily #COVID19  rep...  ...     3        2        4\n",
       "3           3              3  It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...  ...     1        0        0\n",
       "4           4              4  RT @VikiLovesFACS:How can we know that #COVID1...  ...     0        1        0\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('df_sent.csv', encoding='utf8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e7b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_calc(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['sentiment'] = df['cleantext'].apply(sentiment_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "350bbbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>usa</th>\n",
       "      <th>usar</th>\n",
       "      <th>nevent</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>MNB</th>\n",
       "      <th>DENSE</th>\n",
       "      <th>USE</th>\n",
       "      <th>BERT</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @EbonyT_PhD:Shout out to the nurses who??Â€?...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nurse who?Â€ve told can?Â€t drink water nurse st...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @GOPKeegan: True! #Democrats #COVID19 #Trut...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>true democrat covid19 truth covid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @nilikm:In other words, daily #COVID19  rep...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Mississauga, Ontario</td>\n",
       "      <td>1280000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>word daily covid19 repos gross underestimate t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...</td>\n",
       "      <td>Wed Dec 29 18:01:41 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>at least 2m away from you</td>\n",
       "      <td>26824287.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>it?Â€s wednesday school stas monday one 430pm f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RT @VikiLovesFACS:How can we know that #COVID1...</td>\n",
       "      <td>Wed Dec 29 18:01:42 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1330000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>il</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>know covid19 vaccine harm feility could effect...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103619</th>\n",
       "      <td>103619</td>\n",
       "      <td>103615</td>\n",
       "      <td>RT @HeadUNDRR: There may be more to come in wh...</td>\n",
       "      <td>Mon Aug 31 16:07:27 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>Newcastle Upon Tyne, England</td>\n",
       "      <td>285463199.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>may come already active atlantic hurricane sea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103620</th>\n",
       "      <td>103620</td>\n",
       "      <td>103616</td>\n",
       "      <td>No one should wish for 4 more years of reckles...</td>\n",
       "      <td>Mon Aug 31 16:07:28 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>880965564.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one wish year reckless endangerment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103621</th>\n",
       "      <td>103621</td>\n",
       "      <td>103617</td>\n",
       "      <td>It's #Tornado &amp;amp; #HurricaneSeason ...Double...</td>\n",
       "      <td>Mon Aug 31 16:07:28 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>South Florida</td>\n",
       "      <td>1681212607.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>florida</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tornado amp hurricaneseason double whammy get ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103622</th>\n",
       "      <td>103622</td>\n",
       "      <td>103618</td>\n",
       "      <td>RT @JoeBiden: There?s a lot happening, but I h...</td>\n",
       "      <td>Mon Aug 31 16:07:30 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>464204423.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>wi</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there lot happening hope dont overlook brave f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103623</th>\n",
       "      <td>103623</td>\n",
       "      <td>103619</td>\n",
       "      <td>RT @jray5900: If you?re from Louisiana and not...</td>\n",
       "      <td>Mon Aug 31 16:07:31 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>ur heart</td>\n",
       "      <td>2609578840.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youre louisiana enraged something wrong trump ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103624 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Unnamed: 0.1.1                                               text  ... negative positive sentiment\n",
       "0                0              0  RT @EbonyT_PhD:Shout out to the nurses who??Â€?...  ...        1        3  0.113636\n",
       "1                1              1  RT @GOPKeegan: True! #Democrats #COVID19 #Trut...  ...        0        2  0.350000\n",
       "2                2              2  RT @nilikm:In other words, daily #COVID19  rep...  ...        2        4  0.000000\n",
       "3                3              3  It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...  ...        0        0 -0.600000\n",
       "4                4              4  RT @VikiLovesFACS:How can we know that #COVID1...  ...        1        0  0.050000\n",
       "...            ...            ...                                                ...  ...      ...      ...       ...\n",
       "103619      103619         103615  RT @HeadUNDRR: There may be more to come in wh...  ...        0        0 -0.133333\n",
       "103620      103620         103616  No one should wish for 4 more years of reckles...  ...        2        0  0.000000\n",
       "103621      103621         103617  It's #Tornado &amp; #HurricaneSeason ...Double...  ...        0        0  0.100000\n",
       "103622      103622         103618  RT @JoeBiden: There?s a lot happening, but I h...  ...        0        1  0.800000\n",
       "103623      103623         103619  RT @jray5900: If you?re from Louisiana and not...  ...        1        0 -0.200000\n",
       "\n",
       "[103624 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3988bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentimentNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f44e2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>usa</th>\n",
       "      <th>usar</th>\n",
       "      <th>nevent</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>MNB</th>\n",
       "      <th>DENSE</th>\n",
       "      <th>USE</th>\n",
       "      <th>BERT</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @EbonyT_PhD:Shout out to the nurses who??Â€?...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>718000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nurse who?Â€ve told can?Â€t drink water nurse st...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.8523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @GOPKeegan: True! #Democrats #COVID19 #Trut...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>true democrat covid19 truth covid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.6588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @nilikm:In other words, daily #COVID19  rep...</td>\n",
       "      <td>Wed Dec 29 18:01:40 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Mississauga, Ontario</td>\n",
       "      <td>1280000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>word daily covid19 repos gross underestimate t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.6440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...</td>\n",
       "      <td>Wed Dec 29 18:01:41 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>at least 2m away from you</td>\n",
       "      <td>26824287.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>it?Â€s wednesday school stas monday one 430pm f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RT @VikiLovesFACS:How can we know that #COVID1...</td>\n",
       "      <td>Wed Dec 29 18:01:42 +0000 2021</td>\n",
       "      <td>en</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>1330000000000000000.0</td>\n",
       "      <td>omicron</td>\n",
       "      <td>il</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>know covid19 vaccine harm feility could effect...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.5873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103619</th>\n",
       "      <td>103619</td>\n",
       "      <td>103615</td>\n",
       "      <td>RT @HeadUNDRR: There may be more to come in wh...</td>\n",
       "      <td>Mon Aug 31 16:07:27 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>Newcastle Upon Tyne, England</td>\n",
       "      <td>285463199.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>may come already active atlantic hurricane sea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.4576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103620</th>\n",
       "      <td>103620</td>\n",
       "      <td>103616</td>\n",
       "      <td>No one should wish for 4 more years of reckles...</td>\n",
       "      <td>Mon Aug 31 16:07:28 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>880965564.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one wish year reckless endangerment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.3535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103621</th>\n",
       "      <td>103621</td>\n",
       "      <td>103617</td>\n",
       "      <td>It's #Tornado &amp;amp; #HurricaneSeason ...Double...</td>\n",
       "      <td>Mon Aug 31 16:07:28 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>South Florida</td>\n",
       "      <td>1681212607.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>florida</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tornado amp hurricaneseason double whammy get ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103622</th>\n",
       "      <td>103622</td>\n",
       "      <td>103618</td>\n",
       "      <td>RT @JoeBiden: There?s a lot happening, but I h...</td>\n",
       "      <td>Mon Aug 31 16:07:30 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>464204423.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>wi</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there lot happening hope dont overlook brave f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>target</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.4922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103623</th>\n",
       "      <td>103623</td>\n",
       "      <td>103619</td>\n",
       "      <td>RT @jray5900: If you?re from Louisiana and not...</td>\n",
       "      <td>Mon Aug 31 16:07:31 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>ur heart</td>\n",
       "      <td>2609578840.0</td>\n",
       "      <td>laura</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youre louisiana enraged something wrong trump ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>not_target</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.7576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103624 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Unnamed: 0.1.1                                               text  ... positive sentiment   vader\n",
       "0                0              0  RT @EbonyT_PhD:Shout out to the nurses who??Â€?...  ...        3  0.113636  0.8523\n",
       "1                1              1  RT @GOPKeegan: True! #Democrats #COVID19 #Trut...  ...        2  0.350000  0.6588\n",
       "2                2              2  RT @nilikm:In other words, daily #COVID19  rep...  ...        4  0.000000 -0.6440\n",
       "3                3              3  It??Â€?s Wednesday. \\r\\n\\r\\nSchool starts on Mo...  ...        0 -0.600000  0.0000\n",
       "4                4              4  RT @VikiLovesFACS:How can we know that #COVID1...  ...        0  0.050000  0.5873\n",
       "...            ...            ...                                                ...  ...      ...       ...     ...\n",
       "103619      103619         103615  RT @HeadUNDRR: There may be more to come in wh...  ...        0 -0.133333  0.4576\n",
       "103620      103620         103616  No one should wish for 4 more years of reckles...  ...        0  0.000000 -0.3535\n",
       "103621      103621         103617  It's #Tornado &amp; #HurricaneSeason ...Double...  ...        0  0.100000  0.4995\n",
       "103622      103622         103618  RT @JoeBiden: There?s a lot happening, but I h...  ...        1  0.800000  0.4922\n",
       "103623      103623         103619  RT @jray5900: If you?re from Louisiana and not...  ...        0 -0.200000 -0.7576\n",
       "\n",
       "[103624 rows x 28 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_calc(text):\n",
    "    try:\n",
    "        return analyzer.polarity_scores(text)['compound']\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['vader'] = df['text'].apply(vader_calc)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
